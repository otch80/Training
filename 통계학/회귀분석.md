# 회귀분석 (Regression Analysis)
* ADsP 자격증 내용 중 다수 발췌

> 정의
- 한개 이상의 독립변수들이 종속변수에 미치는 영향을 추정할 수 있는 통계기법  
-> 결과는 이미 있고 (관측값, 종속변수), 내가 생각했을 때 이게 (독립변수) 조금 관측값이 변하는 원인으로 작용하지 않을까 하는 내용을 검증해서 확인해보고, 예상이 맞으면 미래 예측까지 할 수 있는 과정


> 사용 변수
> -
> - 영향을 받는 변수 (y, 내가 원하는 결과값)
>   - 종속변수, 결과변수, 반응변수
> - 영향을 주는 변수 (x, 영향을 줄거라 의심이 드는 변수)
>   - 독립변수, 예측변수, 설명변수

> 회귀분석 종류
> -
> - 선형회귀
> - 로지스틱 회귀
> - 리지 회귀
> - 라쏘 회귀
> - 다항 회귀  
> -> [참고링크](https://www.appier.com/ko-kr/blog/5-types-of-regression-analysis-and-when-to-use-them)


회귀분석에도 여러 종류가 있지만 우선적으로 선형회귀에 대한 내용을 정리해본다.


> 선형회귀분석의 가정
> -
> 선형회귀(x가 1개) 로써 만족해야 하는 조건(기본가정)은 아래와 같다
> - 선형성
>   - 입력변수와 출력변수의 관계가 선형이어야 한다  
>   -> 곡선 $\not=$ 직선
> - 등분산성
>   - 잔차가 일정하다 (여기서 분산은 잔차의 분산)
>   - 잔차 = 관측치 - 예상치
>   - 이와 관련해서도 좋은 예시가 있어 글을 가져왔다
>   ![등분산성](https://upload.wikimedia.org/wikipedia/en/5/5d/Hsked_residual_compare.svg) - 출처 : [러닝머신의 Train Data Set](https://blog.naver.com/PostView.nhn?blogId=nilsine11202&logNo=221590077004&parentCategoryNo=&categoryNo=29&viewDate=&isShowPopularPosts=false&from=postView)
>   - 아래가 등분산성을 만족하는 경우
>   - 위는 뒤로갈수록 잔차가 커지는 경향을 보임
> - 독립성
>   - 독립변수들 간에 특정한 관계가 없음
>   - 예) 풍향과 풍력발전기의 전력 발전량은 큰 관계가 있음. 이걸 모형이 학습하게 되면 상대적으로 호율이 떨어질 것. 오히려 성능이 저하될 가능성도 존재함
> - 비상관성
>   - 오차들끼리 상관이 없다
>   - ADsP에만 나오는 개념으로 보임
> - 정상성(정규성)
>   - 잔차가 정규분포를 따름

> 선형회귀 수식
> -
> - 단변량 회귀분석
> $$y_i = \beta_0 + \beta_1x_i + \epsilon_i, i=1,2,...,n, \epsilon_i ~ N(0,\sigma^2)$$
> $y_i$ : $i$번째 종속변수 값  
> $x_i$ : $i$번째 독립변수 값  
> $\beta_0$선형 회귀식의 절편  
> $\beta_1$ : 선형 회귀식의 기울기  
> $\epsilon_i$ : 오차항 (정규분포를 따름)

> 회귀계수 ($\beta_0, \beta_1$ 추정)
> -
> - 최소제곱법
>   - <U>오차의 제곱의 합이 **최소**가 되는 값 찾는 과정</U>
>   - $Min S^2 = Min \Sigma^n_{i=1}{\epsilon^2} = Min \Sigma^n_{i=1}{(y_i - b_0 - b_1x_i)^2}$
>   - $y_i = \beta_0 + \beta_1x_i + \epsilon_i$ 수식에서 오차 $\epsilon_i$ 를 구하기 위해 수식을 이항시키면 아래와 같다
>   - $\epsilon_i = y_i - \beta_0 - \beta_1x_i$
>   - 이때 $\beta_1$ 이 0이면 $x$에 아무런 값이나 들어가도 식이 성립할 수 있기때문에 $x$와 $y$ 사이에는 아무런 인과관계가 없다. 따라서 적합된 추정식은 아무 의미가 없게 된다.


> 추정된 값 ($\beta_0, \beta_1$) 검증 방법
> -
> - t-test를 통한 p-value
>   - p-value <= 0.05
> - 결정계수 ($R^2$)
>   - 정의 : 독립변수가 종속변수를 얼마나 설명하는지를 나타낸 지표 (추정된 회귀식이 얼마나 타당한가)
>   - **1에 가까울 수록 회귀모형이 자료를 잘 설명**한다고 한다  
>   (예) R^2 = 0.6  
>   -> 설명률이 60%정도 된다  
>   -> 신뢰가능한 정도로 보는 기준은 없음. 상황에 따라 다르게 적용


> 결정계수
> -
> - 전체제곱합 (Total Sum of Squares, SST) : $\Sigma^n_{i=1}(y_i-\bar{y})^2$
>   - $(관측값 - 관측값 평균)^2$
>   - 예측 전 계산할 수 있는 값
> - 회귀제곱합 (Regressino Sum of Squares, SSR) : $\Sigma^n_{i=1}(\hat{y_i}-\bar{y})^2$
>   - $(예측값 - 관측값 평균)^2$
> - 오차제곱합 (Error Sum of Squares, SSE) : $\Sigma^n_{i=1}({y_i}-\hat{y})^2$
>   - 우리가 일반적으로 이해하는 에러, 잔차
> - 결정계수($R^2$) : 회귀제곱합 / 전체제곱합 ($SSR/SST$) [0,1]
>   - $SSR = SST - SSE$
>   ![결정계수](../img/결정계수.png)

> 수정된 결정계수
> -
> 다변량 회귀분석에는 독립변수의 수가 많아지면서 결정계수가 높아질 확률이 크다.
> - 결정계수가 높아지는 이유는 $R^2$에서는 모든 변수들이 종속변수의 분산을 설명하는데 사용한다고 여기기 때문이다.
> - 물론 불필요한 정보의 경우 종속변수 설명률을 감소시키기도 한다. 그래서 수정된 $R^2$를 통해 **실제로** 종속변수에 영향을 주는 독립변수에 의해 설명되는 분산의 비율을 파악한다 (출처 - [Make a dent in the universe](https://chukycheese.github.io/statistics/adjusted-r2/))  
> $$R^2_{adj} = 1 - \frac{(n-1)(1-R^2)}{n-k-1} = 1 - (n-1)\frac{MSE}{SST}$$


> 다변량 회귀분석
> -
> - 다중회귀 (1차 함수)
> $$Y = \beta_0 + \beta_1X_1+\beta_2X_2 + \cdots + \beta_kX_k + \epsilon$$
> 모형의 통계적 유의성은 F-통계량으로 확인함

|요인|제곱합|자유도|제곱평균|F-통계량|
|---|---|---|---|---|
|회귀|회귀제곱합(SSR)|k|MSR=SSR/k|F=MSR/MSE|
|오차|오차제곱합(SSE)|n-k-1|MSE=SSE/(n-k-1)||
|계|전체제곱합(SST)|n-1|||
> 다중공선성 (Multicollonearity)
>   - 독립 변수의 일부가 다른 독립 변수의 조합으로 표현될 수 있는 경우를 말함
>   - 이러한 경우는 결정계수에서 언급한 바와 같이 모델의 설명력을 감소시킬 수 있기 때문에 제거하는것이 좋다
>   - 검사방법
>       - 분산팽창요인 (VIF) : 4보다 크면 다중공선성 존재, 10보다 크면 심각한 문제가 있는 것으로 해석가능
>       - 상태지수 : 10이상이면 문제가 있음, 30보다 크면 심각한 문제가 있다고 해석 가능
>   - 해결방법
>       - 문제가 있는 변수 제거
>       - 주성분회귀, 능형회귀 모형 적용